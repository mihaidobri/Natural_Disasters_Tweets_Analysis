{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweet_lstm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mihaidobri/Natural_Disasters_Tweets_Analysis/blob/master/tweet_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gLfmOXPvKwh",
        "colab_type": "text"
      },
      "source": [
        "## lstm\n",
        "\n",
        "\n",
        "\n",
        "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
        "\n",
        "https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "\n",
        "https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "\n",
        "\n",
        "Glove: https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "\n",
        "[Keras Tokenizer](https://keras.io/preprocessing/text/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jphPKT5v11h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "clear_output()\n",
        "\n",
        "%cd /gdrive/My\\ Drive\n",
        "\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHKYQvCprOZO",
        "colab_type": "code",
        "outputId": "a5eddf73-d7c5-43d8-e1c5-c580e6c196e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# used to clean console\n",
        "from IPython.display import clear_output\n",
        "\n",
        "!wget https://www.dropbox.com/s/z7iv4o9bxtll3w1/sandy_glove_token.csv\n",
        "clear_output()\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "clear_output()\n",
        "\n",
        "!unzip glove.6B.zip\n",
        "clear_output()\n",
        "\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove.6B.zip  sample_data  sandy_glove_token.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k28uShj228Bu",
        "colab_type": "code",
        "outputId": "93e1d1bb-1054-4dc6-a267-cf0a3b510b11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import string\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Flatten, Embedding, LSTM, Conv1D, MaxPooling1D, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BefeY3GEtkQ7",
        "colab_type": "text"
      },
      "source": [
        "### Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khAEs6RuvmT9",
        "colab_type": "code",
        "outputId": "9af4b5b6-c51a-4f9d-b35e-40cd8b3f31f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# read file\n",
        "df = pd.read_csv(\"sandy_glove_token.csv\", usecols=[\"tweet id\", \" tweet\", \" label\", \"tweet_cleaned\"], index_col=None)\n",
        "\n",
        "# print sample\n",
        "print(df[\"tweet_cleaned\"][0])\n",
        "print(df[\" tweet\"][0])\n",
        "print(df[\" label\"][0])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i 've got enough candles to supply a mexican family\n",
            "I've got enough candles to supply a Mexican family\n",
            "off-topic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y68lvvneuHm_",
        "colab_type": "text"
      },
      "source": [
        "### Output of model: on-topic/off-topic -> 1/0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpK4N-VauJyC",
        "colab_type": "code",
        "outputId": "aefb66fe-f1d8-45af-acc9-0d6bf24a1e94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Output labels\n",
        "y = df[\" label\"]\n",
        "print(y[0])\n",
        "\n",
        "# encoding 0-1\n",
        "y = [1 if label==\"on-topic\" else 0 for label in y]\n",
        "print(y[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "off-topic\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DsK3EvjuOi2",
        "colab_type": "text"
      },
      "source": [
        "### Input of model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUPUq-HlzKw2",
        "colab_type": "code",
        "outputId": "fad16f21-1a04-470d-a0c6-5d4578024325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Input\n",
        "X = (df[\"tweet_cleaned\"])\n",
        "\n",
        "print(\"Number of Tweets: \",len(X))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Tweets:  10008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KajZXKnNz8t0",
        "colab_type": "text"
      },
      "source": [
        "### Split to Train Data and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5SeQTdOWTSf",
        "colab_type": "code",
        "outputId": "c0d42a06-326c-48f5-c731-6257e5ee87ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# split train-test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "print(\"Train size:\\t\",len(X_train))\n",
        "print(\"Test size:\\t\", len(X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size:\t 8006\n",
            "Test size:\t 2002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh5fuljJ0AoG",
        "colab_type": "text"
      },
      "source": [
        "### Variables for embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7r9EiJeYLiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# variables \n",
        "top_words = 2000 # most frequet words to use from vocab\n",
        "max_words = max([len(tweet) for tweet in X])    # max number of tweet\n",
        "emb_size = 100    # embedding size\n",
        "emb_file = 'glove.6B.%sd.txt'%(emb_size)   # emb file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqND4Da_0DsK",
        "colab_type": "text"
      },
      "source": [
        "### Tokenizer that translate text to numbers tk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISwu3FoY23HL",
        "colab_type": "code",
        "outputId": "ba971b94-080b-4b8a-8f97-0c9b8a0284a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "# tokenizing tweet\n",
        "tk = Tokenizer(num_words=(top_words), filters=string.punctuation, lower=True, split=' ', char_level=False, oov_token='<unk>')\n",
        "\n",
        "# fit tokenizer\n",
        "for i,tweet in enumerate(X_train):\n",
        "  # feedback of loop - progress\n",
        "  if i%1000 == 0:\n",
        "    print(\"Progress %s of %s\"%(i,len(X_train)))\n",
        "  tk.fit_on_texts([tweet])\n",
        "\n",
        "## **Key Step**\n",
        "tk.word_index = {e:i for e,i in tk.word_index.items() if i <= top_words} # <= because tokenizer is 1 indexed\n",
        "tk.word_index[tk.oov_token] = top_words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Progress 0 of 8006\n",
            "Progress 1000 of 8006\n",
            "Progress 2000 of 8006\n",
            "Progress 3000 of 8006\n",
            "Progress 4000 of 8006\n",
            "Progress 5000 of 8006\n",
            "Progress 6000 of 8006\n",
            "Progress 7000 of 8006\n",
            "Progress 8000 of 8006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "49170d39-5000-457f-c029-f4a002e563f1",
        "id": "76RplKni7Fdl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tk.texts_to_sequences([\"Eu is from the Sinaia\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2000, 14, 55, 5, 2000]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dWa3Akh0K1w",
        "colab_type": "text"
      },
      "source": [
        "### Transform Text Data to sequence of numbers using tk (tokenizer)\n",
        "\n",
        "### Padding to make all tweets same size as max_words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qFGZsJXbQLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = tk.texts_to_sequences(X_train)\n",
        "X_train = pad_sequences(X_train, maxlen=max_words, padding='post')\n",
        "\n",
        "\n",
        "X_test = tk.texts_to_sequences(X_test)\n",
        "X_test = pad_sequences(X_test, maxlen=max_words, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfHFpDzm0gBr",
        "colab_type": "text"
      },
      "source": [
        "### Sample data of text transformed to sequence of numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86BfTeaw4ceU",
        "colab_type": "code",
        "outputId": "442c7f19-eebd-47e8-b235-483472511fa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# sample example\n",
        "print(df[\"tweet_cleaned\"][11])\n",
        "print(tk.texts_to_sequences([df[\"tweet_cleaned\"][11]]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<user> need me to comeback and finish it since i was supposed to sleep over and then i woulda finished\n",
            "[[3, 107, 31, 9, 2000, 18, 1311, 24, 252, 6, 46, 432, 9, 311, 96, 18, 196, 6, 2000, 1877]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esgUN_l-02VQ",
        "colab_type": "text"
      },
      "source": [
        "### Load Glove Embeddings - creates dictionary word-embedding pair"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXs-VCqA9a0x",
        "colab_type": "code",
        "outputId": "1d28c18d-95e3-422f-c977-2ee0ffa86d87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "# load embedding file\n",
        "embeddings_index = dict()\n",
        "\n",
        "# read embedding file\n",
        "with open(emb_file, 'r') as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    emb = np.array(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = emb\n",
        "    \n",
        "\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-00fd401fcf01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# read embedding file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.100d.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGjEAnKV91pL",
        "colab_type": "text"
      },
      "source": [
        "### Embedding Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm5G4mq5-9Op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((top_words+1, emb_size))\n",
        "\n",
        "vocabulary = tk.word_index\n",
        "\n",
        "for word, i in vocabulary.items():\n",
        "  if i == top_words+1:\n",
        "    break  \n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tJMDGmV_2dB",
        "colab_type": "text"
      },
      "source": [
        "### Create LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMmrKLtSN0_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=top_words+1, output_dim=emb_size, weights=[embedding_matrix], input_length=max_words, trainable=False))\n",
        "\n",
        "\n",
        "\"\"\"CNN\"\"\" \n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "\n",
        "\n",
        "\"\"\"CNN + LSTM with dropout\"\"\"\n",
        "# model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "# model.add(MaxPooling1D(pool_size=2))\n",
        "# model.add((LSTM(256, dropout=0.2, recurrent_dropout=0.2)))\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"LSTM with dropout\"\"\"\n",
        "# model.add((LSTM(256, dropout=0.2, recurrent_dropout=0.2)))\n",
        "\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTmLq-MQNfpO",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9xUek8uQJxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,  validation_split=0.15, epochs=20, batch_size=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_PT4BQsNi28",
        "colab_type": "text"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajJVWxyHboBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# round predictions\n",
        "y_pred = [round(x[0]) for x in y_pred]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8o00C61NmFP",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x6M8UqicSwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print('F1 score: %f' % f1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV5PYNShNoPq",
        "colab_type": "text"
      },
      "source": [
        "### Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VySgA37uNq8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GGKmMEMN5Gj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK9ARPxKyq1c",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4McEDoE1j3K",
        "colab_type": "text"
      },
      "source": [
        "## import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-jBHBXKyrHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import string\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Flatten, Embedding, LSTM, Conv1D, MaxPooling1D, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcmCXkf91oxT",
        "colab_type": "text"
      },
      "source": [
        "## Read CSV data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5luTKmlyynq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read file\n",
        "df = pd.read_csv(\"sandy_glove_token.csv\", usecols=[\"tweet id\", \" tweet\", \" label\", \"tweet_cleaned\"], index_col=None)\n",
        "\n",
        "# print sample\n",
        "print(df[\"tweet_cleaned\"][0])\n",
        "print(df[\" tweet\"][0])\n",
        "print(df[\" label\"][0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DRxUfDz1rpQ",
        "colab_type": "text"
      },
      "source": [
        "## Get label output data and encode it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUUBXS0qy1sG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Output labels\n",
        "y = df[\" label\"]\n",
        "print(y[0])\n",
        "\n",
        "# encoding 0-1\n",
        "y = [1 if label==\"on-topic\" else 0 for label in y]\n",
        "print(y[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCoF7UB_1vNQ",
        "colab_type": "text"
      },
      "source": [
        "## Get input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl4J6HWYy7Fg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input\n",
        "X = (df[\"tweet_cleaned\"])\n",
        "\n",
        "print(\"Number of Tweets: \",len(X))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8bWL_k_111h",
        "colab_type": "text"
      },
      "source": [
        "## Split data into Train - Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_ucisejzAxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split train-test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "print(\"Train size:\\t\",len(X_train))\n",
        "print(\"Test size:\\t\", len(X_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yORrkMIy16TD",
        "colab_type": "text"
      },
      "source": [
        "## Get Maximum tweet length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmMivBmDzrzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# variables \n",
        "max_words = max([len(tweet) for tweet in X])    # max number of tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIPuJ4xL2CBD",
        "colab_type": "text"
      },
      "source": [
        "## Fit Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQJubD5CzDyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizing tweet\n",
        "tk = Tokenizer(filters=string.punctuation, lower=True, split=' ', char_level=False, oov_token='<unk>')\n",
        "\n",
        "# fit tokenizer\n",
        "for i,tweet in enumerate(X_train):\n",
        "  # feedback of loop - progress\n",
        "  if i%1000 == 0:\n",
        "    print(\"Progress %s of %s\"%(i,len(X_train)))\n",
        "  tk.fit_on_texts([tweet])\n",
        "\n",
        "## **Key Step**\n",
        "tk.word_index = {e:i for e,i in tk.word_index.items() if i <= top_words} # <= because tokenizer is 1 indexed\n",
        "tk.word_index[tk.oov_token] = top_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNmd8jo_2EjT",
        "colab_type": "text"
      },
      "source": [
        "## Encode training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bKzACULzJHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = tk.texts_to_sequences(X_train)\n",
        "X_train = pad_sequences(X_train, maxlen=max_words, padding='post')\n",
        "\n",
        "\n",
        "X_test = tk.texts_to_sequences(X_test)\n",
        "X_test = pad_sequences(X_test, maxlen=max_words, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrrU8PJm2M3v",
        "colab_type": "text"
      },
      "source": [
        "## Print Sample code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13thDos4zN7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample example\n",
        "print(df[\"tweet_cleaned\"][11])\n",
        "print(tk.texts_to_sequences([df[\"tweet_cleaned\"][11]]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n_uWE9U2Oxn",
        "colab_type": "text"
      },
      "source": [
        "# Create Gaussian NB and Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq8tv2nkzUSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = GaussianNB()\n",
        "\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5slupsRK2SRu",
        "colab_type": "text"
      },
      "source": [
        "## User Gaussian NB to make predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP1xsnfj0XVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofATwleM2amy",
        "colab_type": "text"
      },
      "source": [
        " # Evaluation On Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNqEK7N-0mpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print('F1 score: %f' % f1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}